{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis can be thought of as the exercise of taking a sentence, paragraph, document, or any piece of natural language, and determining whether that text's emotional tone is positive, negative or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common theme is that the inputs need to be scalar values, or matrices of scalar values. When you think of sentiment analysis (as NLP task), however, a data pipeline like this may come to mind. \n",
    " \n",
    "![caption](Images/SentimentAnalysis.png)\n",
    "\n",
    "This kind of pipeline is problematic. There is no way for us to do common operations like dot products or backpropagation on a single string. Instead of having a string input, we will need to convert each word in the sentence to a vector. \n",
    "\n",
    "![caption](Images/SentimentAnalysis2.png)\n",
    "\n",
    "You can think of the input to the sentiment analysis module as being a 16 x D dimensional matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want these vectors to be created in such a way that they somehow represent the word and its context, meaning, and semantics. For example, we’d like the vectors for the words “love” and “adore” to reside in relatively the same area in the vector space since they both have similar definitions and are both used in similar contexts. The vector representation of a word is also known as a word embedding.\n",
    "\n",
    "![caption](Images/SentimentAnalysis8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create these word embeddings, we'll use a model that's commonly reffered to as \"Word2Vec\". \n",
    "\n",
    "The output of a Word2Vec model is called an embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the actual network architecture we're going to be building. \n",
    "Each word in a sentence depends greatly on what came before and comes after it. \n",
    "In order to account for this dependency, we use a recurrent neural network.  \n",
    "\n",
    "![caption](Images/SentimentAnalysis18.png)\n",
    "\n",
    "The main difference between feedforward neural networks and recurrent ones is the temporal aspect of the latter. In RNNs, each word in an input sequence will be associated with a specific time step. In effect, the number of time steps will be equal to the max sequence length. \n",
    "\n",
    "![caption](Images/SentimentAnalysis16.png)\n",
    "\n",
    "The hidden state is a function of both the current word vector and the hidden state vector at the previous time step. The sigma indicates that the sum of the two terms will be put through an activation function (normally a sigmoid or tanh).\n",
    "\n",
    "![caption](Images/SentimentAnalysis15.png) \n",
    "\n",
    "The weight matrices are updated through an optimization process called backpropagation through time. \n",
    "\n",
    "The hidden state vector at the final time step is fed into a binary softmax classifier where it is multiplied by another weight matrix and put through a softmax function that outputs values between 0 and 1, effectively giving us the probabilities of positive and negative sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory Units (LSTMs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the middle sentence had no impact? However, there is a strong connection between the first and third sentences. LSTMs help in handling long term dependencies.\n",
    "\n",
    "The computation is broken up into 4 components, an input gate, a forget gate, an output gate, and a new memory container. \n",
    "\n",
    "![caption](Images/SentimentAnalysis10.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing Sentiment Analysis as a Deep Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the task of sentiment analysis involves taking in an input sequence of words and determining whether the sentiment is positive, negative, or neutral. We can separate this specific task (and most other NLP tasks) into 5 different components.\n",
    "\n",
    "    1) Training a word vector generation model (such as Word2Vec) or loading pretrained word vectors\n",
    "    2) Creating an ID's matrix for our training set (We'll discuss this a bit later)\n",
    "    3) RNN (With LSTM units) graph creation\n",
    "    4) Training \n",
    "    5) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to create our word vectors. For simplicity, we're going to be using a pretrained model. \n",
    "\n",
    "We'd use those vectors - matrix that is trained using [GloVe](http://nlp.stanford.edu/projects/glove/), a similar word vector generation model. The matrix will contain 400,000 word vectors, each with a dimensionality of 50. \n",
    "\n",
    "We're going to be importing two different data structures, one will be a Python list with the 400,000 words, and one will be a 400,000 x 50 dimensional embedding matrix that holds all of the word vector values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] \n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the vocabulary list and the embedding matrix. \n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62231 ,  1.1986  , -0.014116,  0.20125 ,  0.69419 ,  0.12068 ,\n",
       "       -0.90399 , -1.4023  ,  0.43357 , -0.48537 , -0.4645  ,  0.15756 ,\n",
       "        0.54261 , -0.32467 , -0.025646,  0.45742 ,  0.16561 ,  0.18819 ,\n",
       "        0.062099, -0.86418 , -1.0425  , -0.81157 ,  0.3126  , -0.20279 ,\n",
       "        0.55734 , -0.28634 , -0.14874 ,  1.0098  ,  0.25041 , -0.53195 ,\n",
       "        2.3793  , -0.76966 , -0.63219 ,  0.3203  ,  0.15072 ,  0.23326 ,\n",
       "       -0.26254 , -0.29461 ,  0.7671  , -0.11577 , -0.68129 , -0.65413 ,\n",
       "       -0.58914 ,  0.24684 ,  1.5904  ,  0.33025 ,  0.41513 , -1.7468  ,\n",
       "        0.82453 , -1.0886  ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#access its corresponding vector through the embedding matrix. (word: tree)\n",
    "baseballIndex = wordsList.index('tree')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vectors, our first step is taking an input sentence and then constructing the its vector representation. Let's say that we have the input sentence \"I thought the movie was incredible and inspiring\". In order to get the word vectors, we can use Tensorflow's embedding lookup function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saprem/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data pipeline can be illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set we're going to use is the **Imdb movie review dataset**. This set has 25,000 movie reviews, with 12,500 positive reviews and 12,500 negative reviews. Each of the reviews is stored in a txt file that we need to parse through. The positive reviews are stored in one directory and the negative reviews are stored in another. The following piece of code will determine total and average number of words in each review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "('The total number of files is', 25000)\n",
      "('The total number of words in the files is', 5844680)\n",
      "('The average number of words in the files is', 233)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import io\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with io.open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with io.open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Matplot library to visualize this data in a histogram format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58e8140a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram as well as the average number of words per file, we can safely say that most reviews will fall under 250 words, which is the max sequence length value we will set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can take a single file and transform it into our ids matrix. This is what one of the reviews looks like in text file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher King, but its not crap, either. My only complaint is that Brooks should have cast someone else in the lead (I love Mel as a Director and Writer, not so much as a lead).\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3] \n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert to to an ids matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    37,     14,   2407, 201534,     96,  37314,    319,   7158,\n",
       "       201534,   6469,   8828,   1085,     47,   9703,     20,    260,\n",
       "           36,    455,      7,   7284,   1139,      3,  26494,   2633,\n",
       "          203,    197,   3941,  12739,    646,      7,   7284,   1139,\n",
       "            3,  11990,   7792,     46,  12608,    646,      7,   7284,\n",
       "         1139,      3,   8593,     81,  36381,    109,      3, 201534,\n",
       "         8735,    807,   2983,     34,    149,     37,    319,     14,\n",
       "          191,  31906,      6,      7,    179,    109,  15402,     32,\n",
       "           36,      5,      4,   2933,     12,    138,      6,      7,\n",
       "          523,     59,     77,      3, 201534,     96,   4246,  30006,\n",
       "          235,      3,    908,     14,   4702,   4571,     47,     36,\n",
       "       201534,   6429,    691,     34,     47,     36,  35404,    900,\n",
       "          192,     91,   4499,     14,     12,   6469,    189,     33,\n",
       "         1784,   1318,   1726,      6, 201534,    410,     41,    835,\n",
       "        10464,     19,      7,    369,      5,   1541,     36,    100,\n",
       "          181,     19,      7,    410,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if indexCounter < maxSeqLength:\n",
    "            try:\n",
    "                firstFile[indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for each of our 25,000 reviews. We'll load in the movie training set and integerize it to get a 25000 x 250 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in positiveFiles:\n",
    "   with open(pf, \"r\") as f:\n",
    "       indexCounter = 0\n",
    "       line=f.readline()\n",
    "       cleanedLine = cleanSentences(line)\n",
    "       split = cleanedLine.split()\n",
    "       for word in split:\n",
    "           try:\n",
    "               ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "           except ValueError:\n",
    "               ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "           indexCounter = indexCounter + 1\n",
    "           if indexCounter >= maxSeqLength:\n",
    "               break\n",
    "       fileCounter = fileCounter + 1 \n",
    "\n",
    "for nf in negativeFiles:\n",
    "   with open(nf, \"r\") as f:\n",
    "       indexCounter = 0\n",
    "       line=f.readline()\n",
    "       cleanedLine = cleanSentences(line)\n",
    "       split = cleanedLine.split()\n",
    "       for word in split:\n",
    "           try:\n",
    "               ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "           except ValueError:\n",
    "               ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "           indexCounter = indexCounter + 1\n",
    "           if indexCounter >= maxSeqLength:\n",
    "               break\n",
    "       fileCounter = fileCounter + 1 \n",
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S BEGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’re ready to start creating our Tensorflow graph. We’ll first need to define some hyperparameters, such as batch size, number of LSTM units, number of output classes, and number of training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with most Tensorflow graphs, we’ll now need to specify two placeholders, one for the inputs into the network, and one for the labels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels placeholder represents a set of values, each either [1, 0] or [0, 1], depending on whether each training example is positive or negative. Each row in the integerized input placeholder represents the integerized representation of each training example that we include in our batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll feed both the LSTM cell and the 3-D tensor full of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll define correct prediction and accuracy metrics to track how the network is doing. The correct prediction formulation works by looking at the index of the maximum value of the 2 output values, and then seeing whether it matches with the training labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define a standard cross entropy loss with a softmax layer put on top of the final prediction values. For the optimizer, we’ll use Adam and the default learning rate of .001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right values for your hyperparameters is a crucial part of training deep neural networks effectively. \n",
    "\n",
    "* Learning Rate\n",
    "* Optimizer\n",
    "* Number of LSTM units\n",
    "* Word Vector Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models_trained/pretrained_lstm.ckpt-10000\n",
      "saved to models_trained/pretrained_lstm.ckpt-20000\n",
      "saved to models_trained/pretrained_lstm.ckpt-30000\n",
      "saved to models_trained/pretrained_lstm.ckpt-40000\n",
      "saved to models_trained/pretrained_lstm.ckpt-50000\n",
      "saved to models_trained/pretrained_lstm.ckpt-60000\n",
      "saved to models_trained/pretrained_lstm.ckpt-70000\n",
      "saved to models_trained/pretrained_lstm.ckpt-80000\n",
      "saved to models_trained/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models_trained/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for this batch:', 79.16666865348816)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 66.66666865348816)\n",
      "('Accuracy for this batch:', 87.5)\n",
      "('Accuracy for this batch:', 87.5)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 91.66666865348816)\n",
      "('Accuracy for this batch:', 91.66666865348816)\n",
      "('Accuracy for this batch:', 83.33333134651184)\n",
      "('Accuracy for this batch:', 95.83333134651184)\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "inputText = \"That movie was terrible.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "# predictedSentiment[0] represents output score for positive sentiment\n",
    "# predictedSentiment[1] represents output score for negative sentiment\n",
    "\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "secondInputText = \"That movie was the best one I have ever seen.\"\n",
    "secondInputMatrix = getSentenceMatrix(secondInputText)\n",
    "predictedSentiment = sess.run(prediction, {input_data: secondInputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "thirdInputText = \"Justic League was the not as good as expected.\"\n",
    "thirdInputMatrix = getSentenceMatrix(thirdInputText)\n",
    "predictedSentiment = sess.run(prediction, {input_data: thirdInputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "fourthInputText = \"I thought XYZ movie would be not so good but turn out to be excellent.\"\n",
    "fourthInputMatrix = getSentenceMatrix(fourthInputText)\n",
    "predictedSentiment = sess.run(prediction, {input_data: fourthInputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print \"Positive Sentiment\"\n",
    "else:\n",
    "    print \"Negative Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
